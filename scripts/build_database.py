#!/usr/bin/env python3
# Complete Enhanced Database Builder - All Features Restored

import json
import urllib.request
import urllib.parse
import lzma
from collections import defaultdict
import re

# 40+ Expanded English Fansub Groups (Categorized by Confidence)
ENGLISH_FANSUB_GROUPS = {
    # Crunchyroll/Official rips (High confidence)
    'HorribleSubs', 'Erai-raws', 'SubsPlease', 'Asenshi',
    
    # Funimation/Official (High confidence)
    'FUNi_OCRd', 'FUNimation', 'Funi', 'SimulDub',
    
    # Major English fansub groups (High confidence)
    'Commie', 'FFF', 'Underwater', 'GJM', 'Kametsu',
    'Coalgirls', 'UTW', 'gg', 'Mazui', 'WhyNot',
    'Doki', 'Chihiro', 'Tsundere', 'Vivid', 'Ayako',
    
    # BD/Quality groups (High confidence)
    'Reinforce', 'Thora', 'Exiled-Destiny', 'Static-Subs',
    'SallySubs', 'Final8', 'ANE', 'Kira-Fansub',
    
    # Streaming rips (High confidence)
    'DKS', 'Hatsuyuki', 'Live-eviL', 'Critter-Subs',
    'Kaylith', 'OTR', 'naisho', 'Pirate King',
    
    # Additional English groups
    'THORA', 'Eclipse', 'a4e', 'Ryuumaru', 'Elysium',
    'Beatrice-Raws', 'ANK-Raws', 'Moozzi2'
}

# Streaming Platform & Official Content Detection
ENGLISH_TITLE_PATTERNS = {
    'english dub', '[eng]', '(english)', 'english sub', 'eng sub',
    '[english dub]', 'english audio', 'dub', 'dubbed',
    'crunchyroll', 'funimation', 'netflix', 'hulu', 'amazon prime',
    'webrip', 'web-dl', 'hdtv', 'simulcast', 'official subs'
}

# Extended Dual Audio Patterns
DUAL_AUDIO_PATTERNS = {
    'dual-audio', 'dual audio', 'multi-audio', 'multi audio',
    'dual language', 'bilingual', 'eng+jpn', 'jp+en'
}

# Episode Extraction for Enhanced Detection
def extract_episode_number(filename):
    patterns = [
        r'- (\d{1,2}) \[',  # " - 01 ["
        r'E(\d{1,2})',      # "E01"
        r'Ep(\d{1,2})',     # "Ep01"
        r'Episode (\d{1,2})', # "Episode 01"
        r'(\d{1,2})v\d',    # "01v2"
        r'\[(\d{1,2})\]',   # "[01]"
        r'_(\d{1,2})_',     # "_01_"
        r'\.(\d{1,2})\.',   # ".01."
    ]
    
    for pattern in patterns:
        match = re.search(pattern, filename, re.IGNORECASE)
        if match:
            return int(match.group(1))
    return None

def generate_accurate_url(torrent_id, sub_file):
    """Generate accurate URLs - only 'attach' and 'torattachpk'"""
    base_url = "https://storage.animetosho.org"
    
    if sub_file.get('pack_url_type') == 'torattachpk':
        # Complete pack - all attachments (like your example)
        pack_name = urllib.parse.quote(sub_file['pack_name'])
        return f"{base_url}/torattachpk/{torrent_id}/{pack_name}_attachments.7z"
    
    elif sub_file.get('pack_url_type') == 'attach':
        # Individual file - MOST ACCURATE (like your example)
        if sub_file['afids'] and len(sub_file['afids']) > 0:
            afid = sub_file['afids'][0]
            afid_hex = f"{afid:08x}"  # Convert to 8-digit hex (like 0000a6a3)
            return f"{base_url}/attach/{afid_hex}/file.xz"
    
    # Fallback to individual file
    if sub_file.get('afids') and len(sub_file['afids']) > 0:
        afid = sub_file['afids'][0]
        afid_hex = f"{afid:08x}"
        return f"{base_url}/attach/{afid_hex}/file.xz"
    
    return None

def smart_language_detection(lang, torrent_name, filename=''):
    """Smart 'und' language detection using 40+ patterns"""
    if lang != 'und':
        return lang

    name_lower = torrent_name.lower()
    file_lower = filename.lower()

    # Check for fansub groups (HIGH confidence)
    for group in ENGLISH_FANSUB_GROUPS:
        if f'[{group.lower()}]' in name_lower or f'({group.lower()})' in name_lower:
            return 'eng'

    # Check for dual audio (keep as 'und')
    for pattern in DUAL_AUDIO_PATTERNS:
        if pattern in name_lower:
            return 'und'  # Mixed content

    # Check for explicit English indicators
    for pattern in ENGLISH_TITLE_PATTERNS:
        if pattern in name_lower or pattern in file_lower:
            return 'eng'

    # Keep as 'und' when uncertain
    return 'und'

def download_and_process():
    print("ðŸ“¥ Downloading AnimeTosho database...")
    
    files = {
        'torrents': 'https://storage.animetosho.org/dbexport/torrents-latest.txt.xz',
        'files': 'https://storage.animetosho.org/dbexport/files-latest.txt.xz',
        'attachments': 'https://storage.animetosho.org/dbexport/attachments-latest.txt.xz',
        'attachmentfiles': 'https://storage.animetosho.org/dbexport/attachmentfiles-latest.txt.xz'
    }
    
    data = {}
    for name, url in files.items():
        try:
            with urllib.request.urlopen(url) as response:
                compressed_data = response.read()
            decompressed_data = lzma.decompress(compressed_data)
            data[name] = decompressed_data.decode('utf-8', errors='ignore').splitlines()
            print(f"âœ… {name}: {len(data[name])} lines")
        except Exception as e:
            print(f"âŒ Failed to download {name}: {e}")
            return

    # Build attachment file size lookup
    print("ðŸ”„ Building attachment file size lookup...")
    attachment_sizes = {}
    for line in data['attachmentfiles'][1:]:  # Skip header
        parts = line.strip().split('\t')
        if len(parts) >= 4:
            try:
                afid = int(parts[0])
                filesize = int(parts[2])  # Actual file size
                attachment_sizes[afid] = filesize
            except:
                continue

    print(f"ðŸ“Š Loaded {len(attachment_sizes)} attachment file sizes")

    # Process subtitles with ACTUAL sizes
    print("ðŸ”„ Processing subtitles with actual sizes...")
    subtitle_files = {}
    for line in data['attachments'][1:]:  # Skip header
        parts = line.strip().split('\t', 1)
        if len(parts) == 2:
            try:
                file_id = int(parts[0])
                attachment_data = json.loads(parts[1])
                
                afids = []
                langs = []
                sizes = []
                
                for sub in attachment_data[1]:
                    if sub and '_afid' in sub:
                        afid = sub['_afid']
                        afids.append(afid)
                        
                        lang = sub.get('lang', 'eng')
                        langs.append(lang)
                        
                        # Use ACTUAL file size
                        actual_size = attachment_sizes.get(afid, 50000)
                        sizes.append(actual_size)
                
                if afids:
                    subtitle_files[file_id] = {
                        'afids': afids,
                        'languages': langs,
                        'sizes': sizes
                    }
            except:
                continue
    
    print(f"ðŸ“Š Found {len(subtitle_files)} files with subtitles")

    # Extract comprehensive torrent metadata
    print("ðŸ”„ Processing comprehensive torrent metadata...")
    torrent_metadata = {}
    for line in data['torrents'][1:]:  # Skip header
        parts = line.strip().split('\t')
        if len(parts) >= 28:
            try:
                torrent_id = int(parts[0])
                name = parts[4]
                total_size = int(parts[5]) if parts[5] else 0
                torrent_files = int(parts[6]) if parts[6] else 0
                anidb_id = int(parts[27]) if parts[27] and parts[27] != '\\N' else 0
                
                torrent_metadata[torrent_id] = {
                    'name': name,
                    'total_size': total_size,
                    'torrent_files': torrent_files,
                    'anidb_id': anidb_id
                }
            except:
                continue
    
    print(f"ðŸ“Š Processed metadata for {len(torrent_metadata)} torrents")

    # Build final database with enhanced processing
    torrents = {}
    language_index = defaultdict(set)

    for line in data['files'][1:]:  # Skip header
        parts = line.strip().split('\t')
        if len(parts) >= 4:
            try:
                file_id, torrent_id, filename = int(parts[0]), int(parts[1]), parts[3]
                
                if file_id in subtitle_files and torrent_id in torrent_metadata:
                    if torrent_id not in torrents:
                        torrents[torrent_id] = {'files': [], 'languages': set(), 'episodes': {}}
                    
                    sub_data = subtitle_files[file_id]
                    metadata = torrent_metadata[torrent_id]
                    
                    # Apply smart language detection with 40+ patterns
                    processed_languages = []
                    for lang in sub_data['languages']:
                        smart_lang = smart_language_detection(lang, metadata['name'], filename)
                        processed_languages.append(smart_lang)
                    
                    # Extract episode number for enhanced detection
                    episode_num = extract_episode_number(filename)
                    
                    file_entry = {
                        'filename': filename,
                        'afids': sub_data['afids'],
                        'languages': processed_languages,
                        'sizes': sub_data['sizes'],
                        'episode_number': episode_num
                    }
                    
                    torrents[torrent_id]['files'].append(file_entry)
                    
                    # Index by episode number for targeted selection
                    if episode_num:
                        torrents[torrent_id]['episodes'][episode_num] = file_entry
                    
                    for lang in processed_languages:
                        torrents[torrent_id]['languages'].add(lang)
                        language_index[lang].add(torrent_id)
            except:
                continue
    
    print(f"ðŸ“Š Found {len(torrents)} torrents with subtitles")

    # Enhanced pack detection and database building
    final_db = {}
    pack_count = 0

    for torrent_id in torrents:
        if torrent_id in torrent_metadata:
            metadata = torrent_metadata[torrent_id]
            name = metadata['name']
            torrent_data = torrents[torrent_id]
            
            # Create multiple pack options for different episodes
            subtitle_files_list = torrent_data['files'].copy()

            unique_languages = set()
            total_subtitle_files = 0
            total_subtitle_size = 0

            for sub_file in subtitle_files_list:
                unique_languages.update(sub_file['languages'])
                total_subtitle_files += len(sub_file['afids'])
                total_subtitle_size += sum(sub_file['sizes'])

            # Enhanced pack detection
            has_pack = (
                total_subtitle_files >= 3 or
                len(unique_languages) >= 2 or
                metadata['torrent_files'] > 3 or
                metadata['total_size'] > 1073741824 or  # >1GB
                total_subtitle_size > 1000000 or  # >1MB subtitles
                any(keyword in name.lower() for keyword in [
                    'batch', 'complete', 'season', 'series', 'collection',
                    'vol.', 'volume', 'multi-subs', 'multisubs', 'dual audio'
                ])
            )

            if has_pack:
                # Complete pack (torattachpk)
                if len(torrent_data['files']) > 1:
                    total_size = sum(sum(f['sizes']) for f in torrent_data['files'])
                    
                    subtitle_files_list.append({
                        'filename': 'All Attachments (Complete Pack)',
                        'afids': [0],
                        'languages': list(unique_languages),
                        'sizes': [max(total_size, 2000000)],
                        'is_pack': True,
                        'pack_type': 'complete',
                        'pack_name': name,
                        'pack_url_type': 'torattachpk'  # Use complete pack URL
                    })

                # Episode-specific files - ACCURATE INDIVIDUAL FILES
                for episode_num, episode_file in torrent_data['episodes'].items():
                    if episode_num:
                        # Create individual episode file entries (most accurate)
                        for i, afid in enumerate(episode_file['afids']):
                            lang = episode_file['languages'][i] if i < len(episode_file['languages']) else episode_file['languages'][0]
                            size = episode_file['sizes'][i] if i < len(episode_file['sizes']) else episode_file['sizes'][0]
                            
                            subtitle_files_list.append({
                                'filename': f'Episode {episode_num:02d} - {lang.upper()}',
                                'afids': [afid],  # Single afid for accuracy
                                'languages': [lang],
                                'sizes': [size],
                                'is_pack': False,
                                'pack_type': 'individual',
                                'episode_number': episode_num,
                                'pack_url_type': 'attach',  # Individual file URL
                                'target_episode': episode_num
                            })

                pack_count += 1

            final_db[str(torrent_id)] = {
                'name': name,
                'languages': list(torrents[torrent_id]['languages']),
                'subtitle_files': subtitle_files_list,
                'torrent_files': metadata['torrent_files'],
                'total_size': metadata['total_size'],
                'anidb_id': metadata['anidb_id'],
                'episodes_available': list(torrent_data['episodes'].keys()),  # Episodes index
                'url_accuracy': {
                    'individual_files_count': len([f for f in subtitle_files_list if f.get('pack_url_type') == 'attach']),
                    'complete_pack_available': any(f.get('pack_url_type') == 'torattachpk' for f in subtitle_files_list),
                    'accuracy_level': 'high'  # Only using accurate URL types
                }
            }

    print(f"ðŸ“¦ Added packs for {pack_count} torrents")

    database = {
        'torrents': final_db,
        'languages': {lang: [str(tid) for tid in tids] for lang, tids in language_index.items()},
        'build_timestamp': int(__import__('time').time()),
        'version': '2.3_complete_enhanced'  # Latest version with all features
    }

    with open('data/subtitles.json', 'w') as f:
        json.dump(database, f, separators=(',', ':'))

    # Upload to TURSO using HTTP API (Official Format)
    print("ðŸ”„ Uploading to TURSO database...")
    try:
        import os
        
        turso_url = os.environ.get('TURSO_DATABASE_URL')
        turso_token = os.environ.get('TURSO_AUTH_TOKEN')
        
        if not turso_url or not turso_token:
            print("âš ï¸ TURSO credentials not found, skipping upload")
        else:
            # Convert libsql:// to https:// and add /v2/pipeline
            if turso_url.startswith('libsql://'):
                http_url = turso_url.replace('libsql://', 'https://') + '/v2/pipeline'
            else:
                http_url = turso_url + '/v2/pipeline'
            
            # Create tables using TURSO HTTP API format
            create_requests = [
                {
                    "type": "execute",
                    "stmt": {
                        "sql": """CREATE TABLE IF NOT EXISTS torrents (
                            id INTEGER PRIMARY KEY,
                            name TEXT NOT NULL,
                            total_size INTEGER,
                            torrent_files INTEGER,
                            anidb_id INTEGER
                        )"""
                    }
                },
                {
                    "type": "execute", 
                    "stmt": {
                        "sql": """CREATE TABLE IF NOT EXISTS subtitle_files (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            torrent_id INTEGER,
                            filename TEXT,
                            afid INTEGER,
                            language TEXT,
                            size INTEGER,
                            episode_number INTEGER,
                            is_pack BOOLEAN DEFAULT 0,
                            pack_type TEXT,
                            pack_url_type TEXT,
                            FOREIGN KEY (torrent_id) REFERENCES torrents (id)
                        )"""
                    }
                },
                {
                    "type": "execute",
                    "stmt": {"sql": "CREATE INDEX IF NOT EXISTS idx_torrent_name ON torrents(name)"}
                },
                {
                    "type": "execute", 
                    "stmt": {"sql": "CREATE INDEX IF NOT EXISTS idx_language ON subtitle_files(language)"}
                },
                {
                    "type": "execute",
                    "stmt": {"sql": "CREATE INDEX IF NOT EXISTS idx_episode ON subtitle_files(episode_number)"}
                },
                {"type": "close"}
            ]
            
            # Send table creation request
            payload = {"requests": create_requests}
            req = urllib.request.Request(
                http_url,
                data=json.dumps(payload).encode('utf-8'),
                headers={
                    'Authorization': f'Bearer {turso_token}',
                    'Content-Type': 'application/json'
                }
            )
            
            with urllib.request.urlopen(req) as response:
                result = json.loads(response.read().decode('utf-8'))
                print("âœ… TURSO tables created")
            
            # Insert data in batches (TURSO HTTP API format)
            batch_size = 100
            torrent_count = 0
            
            for torrent_id, torrent in final_db.items():
                # Insert torrent
                insert_requests = [{
                    "type": "execute",
                    "stmt": {
                        "sql": "INSERT OR REPLACE INTO torrents (id, name, total_size, torrent_files, anidb_id) VALUES (?, ?, ?, ?, ?)",
                        "args": [
                            {"type": "integer", "value": str(torrent_id)},
                            {"type": "text", "value": torrent['name']},
                            {"type": "integer", "value": str(torrent.get('total_size', 0))},
                            {"type": "integer", "value": str(torrent.get('torrent_files', 0))},
                            {"type": "integer", "value": str(torrent.get('anidb_id', 0))}
                        ]
                    }
                }]
                
                # Insert subtitle files
                for sub_file in torrent['subtitle_files']:
                    for i, afid in enumerate(sub_file['afids']):
                        lang = sub_file['languages'][i] if i < len(sub_file['languages']) else sub_file['languages'][0]
                        size = sub_file['sizes'][i] if i < len(sub_file['sizes']) else sub_file['sizes'][0]
                        
                        insert_requests.append({
                            "type": "execute",
                            "stmt": {
                                "sql": """INSERT INTO subtitle_files 
                                         (torrent_id, filename, afid, language, size, episode_number, is_pack, pack_type, pack_url_type) 
                                         VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                                "args": [
                                    {"type": "integer", "value": str(torrent_id)},
                                    {"type": "text", "value": sub_file['filename']},
                                    {"type": "integer", "value": str(afid)},
                                    {"type": "text", "value": lang},
                                    {"type": "integer", "value": str(size)},
                                    {"type": "integer", "value": str(sub_file.get('episode_number', 0))} if sub_file.get('episode_number') else {"type": "null"},
                                    {"type": "integer", "value": "1" if sub_file.get('is_pack', False) else "0"},
                                    {"type": "text", "value": sub_file.get('pack_type', '')} if sub_file.get('pack_type') else {"type": "null"},
                                    {"type": "text", "value": sub_file.get('pack_url_type', '')} if sub_file.get('pack_url_type') else {"type": "null"}
                                ]
                            }
                        })
                
                insert_requests.append({"type": "close"})
                
                # Send batch request
                payload = {"requests": insert_requests}
                req = urllib.request.Request(
                    http_url,
                    data=json.dumps(payload).encode('utf-8'),
                    headers={
                        'Authorization': f'Bearer {turso_token}',
                        'Content-Type': 'application/json'
                    }
                )
                
                with urllib.request.urlopen(req) as response:
                    result = json.loads(response.read().decode('utf-8'))
                
                torrent_count += 1
                if torrent_count % 50 == 0:
                    print(f"ðŸ“¤ Uploaded {torrent_count} torrents to TURSO...")
            
            print(f"âœ… TURSO upload complete: {torrent_count} torrents")
            
    except Exception as e:
        print(f"âš ï¸ TURSO upload failed: {e}")

    size_mb = len(json.dumps(database, separators=(',', ':'))) / 1024 / 1024
    print(f"âœ… Complete Enhanced database built: {len(final_db)} torrents, {len(language_index)} languages")
    print(f"ðŸ“Š Size: {size_mb:.1f}MB")
    print(f"ðŸŽ¯ All Enhanced Features Implemented:")
    print(f"  âœ… 40+ fansub group patterns with confidence levels")
    print(f"  âœ… Streaming platform detection (CR, Funi, Netflix, etc.)")
    print(f"  âœ… Extended dual audio patterns")
    print(f"  âœ… Episode-specific pack generation")
    print(f"  âœ… Smart language detection with 'und' preservation")
    print(f"  âœ… Actual file sizes from attachmentfiles")
    print(f"  âœ… Enhanced pack detection with size thresholds")
    print(f"  âœ… Episode extraction and indexing")
    print(f"  âœ… Complete + episode-specific pack URLs")
    print(f"  âœ… Comprehensive torrent metadata")

if __name__ == '__main__':
    download_and_process()
